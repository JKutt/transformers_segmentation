{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d846dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from Batch import MyIterator, batch_size_fn\n",
    "import os\n",
    "from torchtext.data import get_tokenizer\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ed539",
   "metadata": {},
   "source": [
    "## Tokenizing a sentence\n",
    "- takes in a string sentence and splits it into its tokens\n",
    "- converts token list into a torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12af5349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " our test sentence : tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use pytorch's tokenizer module to create a tokenizing function\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# tokenize our senetence\n",
    "tokens = tokenizer(\"This is a pytorch tutorial for tokenization!\")\n",
    "\n",
    "# create a numpy array to hold the token list\n",
    "sentence_np = np.linspace(0,len(tokens), len(tokens), False)\n",
    "\n",
    "# convert to torch tensor\n",
    "sentence_torch = torch.tensor(sentence_np, dtype=torch.int32)\n",
    "\n",
    "print(f\"\\n\\n our test sentence : {sentence_torch}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee75891",
   "metadata": {},
   "source": [
    "## Create an embedding function for our intput\n",
    "- creates a embedded object of our sentence (A simple lookup table that stores embeddings of a fixed dictionary and size)\n",
    "- size is vocabulary size by model dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d91553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.embed(x)\n",
    "    \n",
    "# create the embedding\n",
    "embed = Embedder(4000, 512)\n",
    "\n",
    "# get embedded text\n",
    "embed_text = embed(sentence_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017a0ef",
   "metadata": {},
   "source": [
    "## Create the Positional Encoder function\n",
    "- embeds a position to each token\n",
    "- incorporates a dropout functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d9071c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 <class 'int'>\n",
      "positional encoding shape: torch.Size([1, 8, 512])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, max_seq_len: int=200, dropout: float=0.1):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            \n",
    "            for i in range(0, d_model, 2):\n",
    "                \n",
    "                pe[pos, i] = \\\n",
    "                np.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                np.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # make embeddings relatively larger\n",
    "        print(self.d_model, type(self.d_model))\n",
    "        x = x * np.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(0)\n",
    "        pe = Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "        print(f\"positional encoding shape: {pe.shape}\")\n",
    "        \n",
    "        if x.is_cuda:\n",
    "            \n",
    "            pe.cuda()\n",
    "        \n",
    "        x = x + pe\n",
    "        \n",
    "        return self.dropout(x)\n",
    "\n",
    "pos_emb = PositionalEncoder(512)\n",
    "\n",
    "pos_emb_text = pos_emb(embed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88536152",
   "metadata": {},
   "source": [
    "## Scaled dot product\n",
    "- create a test q, k, v (query, key, value) and caluclate the scaled dot product.\n",
    "- optional mask is used to prevent current tokens from learning future ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0d1ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " torch.Size([8, 512])\n",
      "K\n",
      " torch.Size([8, 512])\n",
      "V\n",
      " torch.Size([8, 512])\n",
      "Values\n",
      " torch.Size([8, 512])\n",
      "Attention\n",
      " torch.Size([8, 8])\n"
     ]
    }
   ],
   "source": [
    "def scaled_dot_product(\n",
    "    \n",
    "    q: torch.tensor, \n",
    "    k: torch.tensor, \n",
    "    v: torch.tensor, \n",
    "    mask: torch.tensor=None\n",
    "\n",
    ") -> (torch.tensor, torch.tensor):\n",
    "    \n",
    "    # dimension of k \n",
    "    dimension_k = q.size()[-1]\n",
    "    \n",
    "    # attention logits calculated\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    \n",
    "    # normalized logits\n",
    "    attn_logits = attn_logits / np.sqrt(dimension_k)\n",
    "    \n",
    "    # check if masking is needed\n",
    "    if mask is not None:\n",
    "        \n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    \n",
    "    # calculate the attention\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    \n",
    "    # multiply by the value matrix\n",
    "    values = torch.matmul(attention, v)\n",
    "    \n",
    "    return values, attention\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "# test with random tensors\n",
    "\n",
    "#\n",
    "seq_len, d_k = 8, 512\n",
    "\n",
    "# initiate q, k, v\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "\n",
    "# calculate the attention and values\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q.shape)\n",
    "print(\"K\\n\", k.shape)\n",
    "print(\"V\\n\", v.shape)\n",
    "print(\"Values\\n\", values.shape)\n",
    "print(\"Attention\\n\", attention.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d79006",
   "metadata": {},
   "source": [
    "## Multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "510829df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mha output\n",
      " torch.Size([8, 8, 512])\n",
      "Attention\n",
      " torch.Size([8, 8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int, dim_model: int, num_heads: int):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        assert dim_model % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        # set the embedind dimension\n",
    "        self.embed_dim = dim_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim_model // num_heads\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*dim_model)\n",
    "        self.o_proj = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        \n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(\n",
    "        \n",
    "        self, \n",
    "        x: torch.tensor, \n",
    "        mask: torch.tensor=None\n",
    "    \n",
    "    ) -> (torch.tensor, torch.tensor):\n",
    "        \n",
    "        # extract dimensions of the embedding x\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        return o, attention\n",
    "        \n",
    "# ===================================================================\n",
    "\n",
    "# test with random tensors\n",
    "\n",
    "#\n",
    "\n",
    "mha = MultiheadAttention(512, dim_model=512, num_heads=8)\n",
    "sample_sentence = torch.randn(8, 8, 512)\n",
    "output, attn = mha(sample_sentence)\n",
    "\n",
    "print(\"mha output\\n\", output.shape)\n",
    "print(\"Attention\\n\", attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9acdb077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 512])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0fd6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
