{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch.optim.lr_scheduler import MultiplicativeLR, LambdaLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from lpips import LPIPS\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from einops import rearrange\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_prob_std(t, sigma, device='cpu'):\n",
    "  \"\"\"Compute the mean and standard deviation of $p_{0t}(x(t) | x(0))$.\n",
    "\n",
    "  Args:    \n",
    "    t: A vector of time steps.\n",
    "    sigma: The $\\sigma$ in our SDE.  \n",
    "  \n",
    "  Returns:\n",
    "    The standard deviation.\n",
    "  \"\"\"    \n",
    "  t = torch.tensor(t, device=device)\n",
    "  return torch.sqrt((sigma**(2 * t) - 1.) / 2. / np.log(sigma))\n",
    "\n",
    "\n",
    "def diffusion_coeff(t, sigma, device='cpu'):\n",
    "  \"\"\"Compute the diffusion coefficient of our SDE.\n",
    "\n",
    "  Args:\n",
    "    t: A vector of time steps.\n",
    "    sigma: The $\\sigma$ in our SDE.\n",
    "  \n",
    "  Returns:\n",
    "    The vector of diffusion coefficients.\n",
    "  \"\"\"\n",
    "  return torch.tensor(sigma**t, device=device)\n",
    "\n",
    "\n",
    "def Euler_Maruyama_sampler(score_model, \n",
    "              marginal_prob_std,\n",
    "              diffusion_coeff, \n",
    "              batch_size=64, \n",
    "              x_shape=(1, 28, 28),\n",
    "              num_steps=500, \n",
    "              device='cuda', \n",
    "              eps=1e-3, y=None):\n",
    "  \"\"\"Generate samples from score-based models with the Euler-Maruyama solver.\n",
    "\n",
    "  Args:\n",
    "    score_model: A PyTorch model that represents the time-dependent score-based model.\n",
    "    marginal_prob_std: A function that gives the standard deviation of\n",
    "      the perturbation kernel.\n",
    "    diffusion_coeff: A function that gives the diffusion coefficient of the SDE.\n",
    "    batch_size: The number of samplers to generate by calling this function once.\n",
    "    num_steps: The number of sampling steps. \n",
    "      Equivalent to the number of discretized time steps.\n",
    "    device: 'cuda' for running on GPUs, and 'cpu' for running on CPUs.\n",
    "    eps: The smallest time step for numerical stability.\n",
    "  \n",
    "  Returns:\n",
    "    Samples.    \n",
    "  \"\"\"\n",
    "  t = torch.ones(batch_size, device=device)\n",
    "  init_x = torch.randn(batch_size, *x_shape, device=device) \\\n",
    "    * marginal_prob_std(t)[:, None, None, None]\n",
    "  time_steps = torch.linspace(1., eps, num_steps, device=device)\n",
    "  step_size = time_steps[0] - time_steps[1]\n",
    "  x = init_x\n",
    "  with torch.no_grad():\n",
    "    for time_step in tqdm(time_steps):      \n",
    "      batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
    "      g = diffusion_coeff(batch_time_step)\n",
    "      mean_x = x + (g**2)[:, None, None, None] * score_model(x, batch_time_step, y=y) * step_size\n",
    "      x = mean_x + torch.sqrt(step_size) * g[:, None, None, None] * torch.randn_like(x)      \n",
    "  # Do not include any noise in the last sampling step.\n",
    "  return mean_x\n",
    "\n",
    "\n",
    "class WordEmbed(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_dim):\n",
    "    super(WordEmbed, self).__init__()\n",
    "    self.embed = nn.Embedding(vocab_size+1, embed_dim)\n",
    "  \n",
    "  def forward(self, ids):\n",
    "    return self.embed(ids)\n",
    "\n",
    "\n",
    "class GaussianFourierProjection(nn.Module):\n",
    "  \"\"\"Gaussian random features for encoding time steps.\"\"\"  \n",
    "  def __init__(self, embed_dim, scale=30.):\n",
    "    super().__init__()\n",
    "    # Randomly sample weights (frequencies) during initialization. \n",
    "    # These weights (frequencies) are fixed during optimization and are not trainable.\n",
    "    self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "  def forward(self, x):\n",
    "    # Cosine(2 pi freq x), Sine(2 pi freq x)\n",
    "    x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
    "    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "  \"\"\"A fully connected layer that reshapes outputs to feature maps.\n",
    "  Allow time repr to input additively from the side of a convolution layer.\n",
    "  \"\"\"\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.dense = nn.Linear(input_dim, output_dim)\n",
    "  def forward(self, x):\n",
    "    return self.dense(x)[..., None, None] \n",
    "    # this broadcast the 2d tensor to 4d, add the same value across space. \n",
    "  \n",
    "class CrossAttention(nn.Module):\n",
    "  def __init__(self, embed_dim, hidden_dim, context_dim=None, num_heads=1,):\n",
    "    \"\"\"\n",
    "    Note: For simplicity reason, we just implemented 1-head attention. \n",
    "    Feel free to implement multi-head attention! with fancy tensor manipulations.\n",
    "    \"\"\"\n",
    "    super(CrossAttention, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.context_dim = context_dim\n",
    "    self.embed_dim = embed_dim\n",
    "    self.query = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "    if context_dim is None:\n",
    "      self.self_attn = True \n",
    "      self.key = nn.Linear(hidden_dim, embed_dim, bias=False)     ###########\n",
    "      self.value = nn.Linear(hidden_dim, hidden_dim, bias=False)  ############\n",
    "    else:\n",
    "      self.self_attn = False \n",
    "      self.key = nn.Linear(context_dim, embed_dim, bias=False)   #############\n",
    "      self.value = nn.Linear(context_dim, hidden_dim, bias=False) ############\n",
    "    \n",
    "    \n",
    "  def forward(self, tokens, context=None):\n",
    "    # tokens: with shape [batch, sequence_len, hidden_dim]\n",
    "    # context: with shape [batch, contex_seq_len, context_dim]\n",
    "    if self.self_attn:\n",
    "        Q = self.query(tokens)\n",
    "        K = self.key(tokens)  \n",
    "        V = self.value(tokens) \n",
    "    else:\n",
    "        # implement Q, K, V for the Cross attention \n",
    "        Q = self.query(tokens)\n",
    "        K = self.key(context)  \n",
    "        V = self.value(context) \n",
    "    #print(Q.shape, K.shape, V.shape)\n",
    "    ####### YOUR CODE HERE (2 lines)\n",
    "    scoremats = torch.einsum(\"bij,bkj->bik\" ,Q , K)     # inner product of Q and K, a tensor \n",
    "    attnmats = F.softmax(scoremats/(len(Q)**0.5))          # softmax of scoremats\n",
    "    #print(scoremats.shape, attnmats.shape, )\n",
    "    ctx_vecs = torch.einsum(\"BTS,BSH->BTH\", attnmats, V)  # weighted average value vectors by attnmats\n",
    "    return ctx_vecs\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "  \"\"\"The transformer block that combines self-attn, cross-attn and feed forward neural net\"\"\"\n",
    "  def __init__(self, hidden_dim, context_dim):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.attn_self = CrossAttention(hidden_dim, hidden_dim, )\n",
    "    self.attn_cross = CrossAttention(hidden_dim, hidden_dim, context_dim)\n",
    "\n",
    "    self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "    self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "    self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "    # implement a 2 layer MLP with K*hidden_dim hidden units, and nn.GeLU nonlinearity #######\n",
    "    K = 4\n",
    "    self.ffn  = nn.Sequential(\n",
    "        # YOUR CODE HERE ##################\n",
    "        nn.Linear(hidden_dim, K * hidden_dim),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(K * hidden_dim, hidden_dim)\n",
    "    )\n",
    "    \n",
    "  def forward(self, x, context=None):\n",
    "    # Notice the + x as residue connections\n",
    "    x = self.attn_self(self.norm1(x)) + x\n",
    "    # Notice the + x as residue connections\n",
    "    x = self.attn_cross(self.norm2(x), context=context) + x\n",
    "    # Notice the + x as residue connections\n",
    "    x = self.ffn(self.norm3(x)) + x\n",
    "    return x \n",
    "\n",
    "\n",
    "class SpatialTransformer(nn.Module):\n",
    "  def __init__(self, hidden_dim, context_dim):\n",
    "    super(SpatialTransformer, self).__init__()\n",
    "    self.transformer = TransformerBlock(hidden_dim, context_dim)\n",
    "\n",
    "  def forward(self, x, context=None):\n",
    "    b, c, h, w = x.shape\n",
    "    x_in = x\n",
    "    # Combine the spatial dimensions and move the channel dimen to the end\n",
    "    x = rearrange(x, \"b c h w->b (h w) c\")\n",
    "    # Apply the sequence transformer\n",
    "    x = self.transformer(x, context)\n",
    "    # Reverse the process\n",
    "    x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "    # Residue \n",
    "    return x + x_in\n",
    "\n",
    "class UNet_Tranformer(nn.Module):\n",
    "    \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, marginal_prob_std, channels=[32, 64, 128, 256], embed_dim=256, \n",
    "                text_dim=256, nClass=10):\n",
    "        \"\"\"Initialize a time-dependent score-based network.\n",
    "\n",
    "        Args:\n",
    "        marginal_prob_std: A function that takes time t and gives the standard\n",
    "            deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
    "        channels: The number of channels for feature maps of each resolution.\n",
    "        embed_dim: The dimensionality of Gaussian random feature embeddings of time.\n",
    "        text_dim:  the embedding dimension of text / digits. \n",
    "        nClass:    number of classes you want to model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Gaussian random feature embedding layer for time\n",
    "        self.time_embed = nn.Sequential(\n",
    "            GaussianFourierProjection(embed_dim=embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "            )\n",
    "        # Encoding layers where the resolution decreases\n",
    "        self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
    "        self.dense1 = Dense(embed_dim, channels[0])\n",
    "        self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
    "        self.dense2 = Dense(embed_dim, channels[1])\n",
    "        self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
    "        self.dense3 = Dense(embed_dim, channels[2])\n",
    "        self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "        self.attn3 = SpatialTransformer(channels[2], text_dim) \n",
    "        \n",
    "        self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
    "        self.dense4 = Dense(embed_dim, channels[3])\n",
    "        self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])    \n",
    "        # YOUR CODE: interleave some attention layers with conv layers\n",
    "        self.attn4 = SpatialTransformer(channels[3], text_dim) \n",
    "\n",
    "        # Decoding layers where the resolution increases\n",
    "        self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n",
    "        self.dense5 = Dense(embed_dim, channels[2])\n",
    "        self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])   \n",
    "\n",
    "        self.tconv3 = nn.ConvTranspose2d(channels[2], channels[1], 3, stride=2, bias=False, output_padding=1)     #  + channels[2]\n",
    "        self.dense6 = Dense(embed_dim, channels[1])\n",
    "        self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "        \n",
    "        self.tconv2 = nn.ConvTranspose2d(channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)     #  + channels[1]\n",
    "        self.dense7 = Dense(embed_dim, channels[0])\n",
    "        self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
    "        self.tconv1 = nn.ConvTranspose2d(channels[0], 1, 3, stride=1) #  + channels[0]\n",
    "        \n",
    "        # The swish activation function\n",
    "        self.act = nn.SiLU() # lambda x: x * torch.sigmoid(x)\n",
    "        self.marginal_prob_std = marginal_prob_std\n",
    "        self.cond_embed = nn.Embedding(nClass, text_dim)\n",
    "    \n",
    "    def forward(self, x, t, y=None): \n",
    "        # Obtain the Gaussian random feature embedding for t   \n",
    "        embed = self.act(self.time_embed(t))    \n",
    "        y_embed = self.cond_embed(y).unsqueeze(1)\n",
    "        # Encoding path\n",
    "        h1 = self.conv1(x) + self.dense1(embed) \n",
    "        ## Incorporate information from t\n",
    "        ## Group normalization\n",
    "        h1 = self.act(self.gnorm1(h1))\n",
    "        h2 = self.conv2(h1) + self.dense2(embed)\n",
    "        h2 = self.act(self.gnorm2(h2))\n",
    "        h3 = self.conv3(h2) + self.dense3(embed)\n",
    "        h3 = self.act(self.gnorm3(h3))\n",
    "        h3 = self.attn3(h3, y_embed) # Use your attention layers\n",
    "        h4 = self.conv4(h3) + self.dense4(embed)\n",
    "        h4 = self.act(self.gnorm4(h4))\n",
    "        # Your code: Use your additional attention layers! \n",
    "        h4 = self.attn4(h4, y_embed)       ##################### ATTENTION LAYER COULD GO HERE IF ATTN4 IS DEFINED\n",
    "\n",
    "        # Decoding path\n",
    "        h = self.tconv4(h4) + self.dense5(embed)\n",
    "        ## Skip connection from the encoding path\n",
    "        h = self.act(self.tgnorm4(h))\n",
    "        h = self.tconv3(h + h3) + self.dense6(embed)\n",
    "        h = self.act(self.tgnorm3(h))\n",
    "        h = self.tconv2(h + h2) + self.dense7(embed)\n",
    "        h = self.act(self.tgnorm2(h))\n",
    "        h = self.tconv1(h + h1)\n",
    "\n",
    "        # Normalize output\n",
    "        h = h / self.marginal_prob_std(t)[:, None, None, None]\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initilize new score model...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ckpt.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitilize new score model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m   score_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDataParallel(UNet_Tranformer(marginal_prob_std\u001b[38;5;241m=\u001b[39mmarginal_prob_std_fn))\n\u001b[0;32m---> 13\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mckpt.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     14\u001b[0m score_model\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt)\n\u001b[1;32m     15\u001b[0m digit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;66;03m#@param {'type':'integer'}\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/jresearch/lib/python3.11/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/mambaforge/envs/jresearch/lib/python3.11/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/mambaforge/envs/jresearch/lib/python3.11/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ckpt.pth'"
     ]
    }
   ],
   "source": [
    "## Load the pre-trained checkpoint from disk.\n",
    "device = 'cuda' #@param ['cuda', 'cpu'] {'type':'string'}\n",
    "sigma =  25.0#@param {'type':'number'}\n",
    "marginal_prob_std_fn = functools.partial(marginal_prob_std, sigma=sigma)\n",
    "diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=sigma)\n",
    "\n",
    "continue_training = False #@param {type:\"boolean\"}\n",
    "if not continue_training:\n",
    "  print(\"initilize new score model...\")\n",
    "  score_model = torch.nn.DataParallel(UNet_Tranformer(marginal_prob_std=marginal_prob_std_fn))\n",
    "\n",
    "\n",
    "ckpt = torch.load('ckpt_transformer.pth', map_location=device)\n",
    "score_model.load_state_dict(ckpt)\n",
    "digit = 4 #@param {'type':'integer'}\n",
    "sample_batch_size = 64 #@param {'type':'integer'}\n",
    "num_steps = 250 #@param {'type':'integer'}\n",
    "sampler = Euler_Maruyama_sampler #@param ['Euler_Maruyama_sampler', 'pc_sampler', 'ode_sampler'] {'type': 'raw'}\n",
    "# score_model.eval()\n",
    "## Generate samples using the specified sampler.\n",
    "samples = sampler(score_model, \n",
    "        marginal_prob_std_fn,\n",
    "        diffusion_coeff_fn, \n",
    "        sample_batch_size, \n",
    "        num_steps=num_steps,\n",
    "        device=device,\n",
    "        y=digit*torch.ones(sample_batch_size, dtype=torch.long))\n",
    "\n",
    "## Sample visualization.\n",
    "samples = samples.clamp(0.0, 1.0)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "sample_grid = make_grid(samples, nrow=int(np.sqrt(sample_batch_size)))\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.axis('off')\n",
    "plt.imshow(sample_grid.permute(1, 2, 0).cpu(), vmin=0., vmax=1.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
